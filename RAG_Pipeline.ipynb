{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from langchain_groq import ChatGroq\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = ChatGroq(\n",
    "    temperature=0.5,\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    groq_api_key=groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    \"\"\"Load a document and return the text.\"\"\"\n",
    "    extension = os.path.splitext(file_path)[1].lower()\n",
    "    if extension == \".pdf\":\n",
    "        return PyPDFLoader(file_path).load()\n",
    "    elif extension == \".docx\" or extension == \".doc\":\n",
    "        return Docx2txtLoader(file_path).load()\n",
    "    elif extension == \".txt\":\n",
    "        return TextLoader(file_path).load()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pdf_path = \"temp.pdf\"\n",
    "document_text = load_document(sample_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=768, chunk_overlap=128)\n",
    "text_chunks = text_splitter.split_documents(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/44rrbdlx5vxgbtcv7fhxgd0c0000gn/T/ipykernel_24729/669638979.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/moka/Desktop/TARS/myenv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/moka/Desktop/TARS/myenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=text_chunks, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=client, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}), \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, chain, history):\n",
    "    result = chain({\"question\": question, \"chat_history\": history})\n",
    "    history.append((question, result[\"answer\"]))\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/44rrbdlx5vxgbtcv7fhxgd0c0000gn/T/ipykernel_24729/2412595810.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = chain({\"question\": question, \"chat_history\": history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the document about?\n",
      "A: This document appears to be a collection of references to academic papers in the field of natural language processing (NLP) and machine learning. The references are listed in a specific format, with each reference including the authors, title, publication information, and a unique identifier (e.g. [1], [2], etc.).\n",
      "\n",
      "The papers referenced appear to be related to various topics in NLP, including parsing, machine translation, summarization, and neural networks. The references may be used as a bibliography or a list of sources for a research paper or article on NLP.\n",
      "\n",
      "Q: Summarize the key points from the first section.\n",
      "A: The main topics discussed in the document appear to be:\n",
      "\n",
      "1. The approach taken in the model, which involves self-attention.\n",
      "2. The potential benefits of self-attention, including more interpretable models.\n",
      "3. The training regime for the models, including the training data and batching.\n",
      "\n",
      "It seems that the document is discussing the architecture and training of a machine learning model, specifically focusing on the use of self-attention and its effects on model interpretability.\n",
      "\n",
      "Q: Can you explain how the method works?\n",
      "A: According to the provided context, the self-attention approach in the machine learning model works by using a sinusoidal function to represent the positional encoding (PEpos) of the input sequence. This sinusoidal function corresponds to a geometric progression of wavelengths from 2π to 10000·2π. The model uses this sinusoidal positional encoding to allow it to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\n",
      "\n",
      "This means that the self-attention mechanism in the model is able to learn to focus on different parts of the input sequence based on their relative positions, rather than their absolute positions. This is achieved through the use of the sinusoidal positional encoding, which provides a way for the model to capture the relationships between different parts of the input sequence.\n",
      "\n",
      "The model also experimented with using learned positional embeddings instead of the sinusoidal positional encoding, and found that the two versions produced nearly identical results. However, the sinusoidal version was chosen because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "Q: What are the results mentioned in the document?\n",
      "A: The results mentioned in the document are the perplexities (PPL) and BLEU scores for different variations of the Transformer architecture on the English-to-German translation development set, newstest2013. The results are presented in Table 3 of the document.\n",
      "\n",
      "The specific results mentioned are:\n",
      "\n",
      "* Perplexities (PPL) for different models, ranging from 4.66 to 5.77\n",
      "* BLEU scores for different models, ranging from 23.7 to 26.4\n",
      "\n",
      "These results are presented for different variations of the Transformer architecture, including:\n",
      "\n",
      "* Varying the number of attention heads and attention key and value dimensions (rows A)\n",
      "* Varying the attention key size (rows B)\n",
      "* Using positional embedding instead of sinusoids (row E)\n",
      "* Using different hyperparameters, such as dropout rate and learning rate (rows D)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"What is the document about?\",\n",
    "    \"Summarize the key points from the first section.\",\n",
    "    \"Can you explain how the method works?\",\n",
    "    \"What are the results mentioned in the document?\",\n",
    "]\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "for query in example_queries:\n",
    "    response = ask_question(query, conversational_chain, chat_history)\n",
    "    print(f\"Q: {query}\\nA: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
